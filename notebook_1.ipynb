{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3fb91f",
   "metadata": {},
   "source": [
    "# Notebook 1 â€“ Sentiment-Aware Research Pipeline\n",
    "This notebook is self-contained and only needs the files shipped inside `ds_Advay_Sinha`. Run every cell in order inside Google Colab to regenerate the analytics, CSV outputs, and visuals used in the report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea11ee",
   "metadata": {},
   "source": [
    "**Workflow**\n",
    "1. (Optional) Mount Google Drive if you stored this folder there.\n",
    "2. Install the Python dependencies listed in `requirements.txt`.\n",
    "3. Configure the data root so the notebook knows where `csv_files/` and `outputs/` live.\n",
    "4. Load sentiment + trade data, clean timestamps, engineer features, and merge by nearest timestamp (72h tolerance).\n",
    "5. Train the Logistic Regression, Random Forest, Gradient Boosting, and XGBoost classifiers; evaluate trend/contrarian/leverage strategies; export artifacts back into this folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    print('Drive mounted. Update DATA_ROOT below if needed.')\n",
    "except ModuleNotFoundError:\n",
    "    print('Running outside Colab. Proceed to the next cell.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7cfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff53ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('/content/drive/MyDrive/ds_Advay_Sinha')\n",
    "if not DATA_ROOT.exists():\n",
    "    DATA_ROOT = Path.cwd()\n",
    "CSV_DIR = DATA_ROOT / 'csv_files'\n",
    "OUTPUT_DIR = DATA_ROOT / 'outputs'\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Using data root:', DATA_ROOT)\n",
    "print('CSV directory:', CSV_DIR)\n",
    "print('Outputs directory:', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _to_snake(name: str) -> str:\n",
    "    return name.strip().lower().replace(' ', '_').replace('/', '_')\n",
    "\n",
    "sentiment = pd.read_csv(\n",
    "    CSV_DIR / 'fear_greed_index.csv',\n",
    "    parse_dates=['date'],\n",
    "    infer_datetime_format=True,\n",
    ")\n",
    "sentiment = sentiment.sort_values('date').rename(columns={'date': 'sentiment_time'})\n",
    "sentiment['classification'] = sentiment['classification'].fillna('Neutral')\n",
    "score_map = {'Extreme Fear': -2, 'Fear': -1, 'Neutral': 0, 'Greed': 1, 'Extreme Greed': 2}\n",
    "sentiment['sentiment_score'] = sentiment['classification'].map(score_map).fillna(0)\n",
    "rolling = sentiment['sentiment_score'].rolling(window=5, min_periods=2)\n",
    "sentiment['sentiment_score_z'] = (\n",
    "    sentiment['sentiment_score'] - rolling.mean()\n",
    ") / rolling.std(ddof=0)\n",
    "sentiment['sentiment_score_lag1'] = sentiment['sentiment_score'].shift(1)\n",
    "sentiment['sentiment_score_slope'] = sentiment['sentiment_score'].diff().fillna(0)\n",
    "sentiment['sentiment_score_z'] = sentiment['sentiment_score_z'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "sentiment['sentiment_score_lag1'] = sentiment['sentiment_score_lag1'].fillna(sentiment['sentiment_score'])\n",
    "sentiment['sentiment_score_lag1'] = sentiment['sentiment_score_lag1'].fillna(0)\n",
    "\n",
    "trades = pd.read_csv(CSV_DIR / 'historical_data.csv')\n",
    "trades.columns = [_to_snake(col) for col in trades.columns]\n",
    "trades['timestamp'] = pd.to_datetime(trades['timestamp_ist'], format='%d-%m-%Y %H:%M', errors='coerce')\n",
    "numeric_cols = ['execution_price', 'size_tokens', 'size_usd', 'start_position', 'closed_pnl', 'fee']\n",
    "for col in numeric_cols:\n",
    "    trades[col] = pd.to_numeric(trades[col], errors='coerce')\n",
    "trades['size_usd'] = trades['size_usd'].fillna(0)\n",
    "trades['closed_pnl'] = trades['closed_pnl'].fillna(0)\n",
    "trades['start_position'] = trades['start_position'].fillna(0)\n",
    "trades['roi'] = trades['closed_pnl'] / trades['size_usd'].replace(0, np.nan)\n",
    "trades['roi'] = trades['roi'].fillna(0)\n",
    "trades['pnl_positive'] = (trades['closed_pnl'] > 0).astype(int)\n",
    "trades['leverage_ratio'] = trades['size_usd'] / (trades['start_position'].abs() + 1e-6)\n",
    "trades = trades.sort_values('timestamp').dropna(subset=['timestamp'])\n",
    "print('Loaded', len(trades), 'trades and', len(sentiment), 'sentiment rows.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a102f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_features = [\n",
    "    'classification',\n",
    "    'sentiment_score',\n",
    "    'sentiment_score_lag1',\n",
    "    'sentiment_score_slope',\n",
    "    'sentiment_score_z'\n",
    "]\n",
    "merged = pd.merge_asof(\n",
    "    trades,\n",
    "    sentiment[['sentiment_time'] + sentiment_features],\n",
    "    left_on='timestamp',\n",
    "    right_on='sentiment_time',\n",
    "    direction='backward',\n",
    "    tolerance=pd.Timedelta('72H')\n",
    ")\n",
    "merged = merged.sort_values('timestamp')\n",
    "merged['classification'] = merged['classification'].fillna('Neutral')\n",
    "for col in sentiment_features[1:]:\n",
    "    merged[col] = merged[col].fillna(0)\n",
    "merged['abs_leverage'] = merged['leverage_ratio'].abs().clip(0, 50)\n",
    "merged['notional_risk'] = merged['size_usd'].abs()\n",
    "recent_window = merged['sentiment_score'].rolling(window=96, min_periods=12)\n",
    "merged['recent_sentiment_z'] = (\n",
    "    merged['sentiment_score'] - recent_window.mean()\n",
    ") / recent_window.std(ddof=0)\n",
    "merged['recent_sentiment_z'] = merged['recent_sentiment_z'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "print('Merged dataset shape:', merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = [\n",
    "    'execution_price',\n",
    "    'size_usd',\n",
    "    'start_position',\n",
    "    'fee',\n",
    "    'abs_leverage',\n",
    "    'sentiment_score',\n",
    "    'sentiment_score_lag1',\n",
    "    'sentiment_score_slope',\n",
    "    'sentiment_score_z',\n",
    "    'recent_sentiment_z'\n",
    "]\n",
    "X = merged[feature_cols].fillna(0)\n",
    "y = merged['pnl_positive']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=300, max_depth=12, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        seed=42\n",
    "    )\n",
    "}\n",
    "model_metrics = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    cm = confusion_matrix(y_test, preds).tolist()\n",
    "    model_metrics[name] = {\n",
    "        'accuracy': float(accuracy_score(y_test, preds)),\n",
    "        'f1': float(f1_score(y_test, preds)),\n",
    "        'roc_auc': float(roc_auc_score(y_test, probs)),\n",
    "        'confusion': cm\n",
    "    }\n",
    "(CSV_DIR / 'model_metrics.json').write_text(json.dumps(model_metrics, indent=2))\n",
    "model_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57586af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe(returns: pd.Series) -> float:\n",
    "    std = returns.std()\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    return float((returns.mean() / std) * np.sqrt(252))\n",
    "\n",
    "def sortino(returns: pd.Series) -> float:\n",
    "    downside = returns[returns < 0].std()\n",
    "    if pd.isna(downside) or downside == 0:\n",
    "        return 0.0\n",
    "    return float((returns.mean() / downside) * np.sqrt(252))\n",
    "\n",
    "def profit_factor(returns: pd.Series) -> float:\n",
    "    gains = returns[returns > 0].sum()\n",
    "    losses = returns[returns < 0].sum()\n",
    "    return float(gains / abs(losses)) if losses != 0 else float('inf')\n",
    "\n",
    "def expectancy(returns: pd.Series) -> float:\n",
    "    return float(returns.mean())\n",
    "\n",
    "trend_signal = np.sign(merged['sentiment_score'])\n",
    "contrarian_signal = np.where(merged['classification'].str.contains('Extreme'), -trend_signal, 0)\n",
    "leverage_signal = trend_signal * (1 + merged['abs_leverage'].clip(0, 5) / 5)\n",
    "\n",
    "returns = {\n",
    "    'Trend Following': merged['roi'] * trend_signal,\n",
    "    'Contrarian Extremes': merged['roi'] * contrarian_signal,\n",
    "    'Leverage Scaling': merged['roi'] * leverage_signal\n",
    "}\n",
    "strategy_metrics = {}\n",
    "for name, ret in returns.items():\n",
    "    ser = pd.Series(ret).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    if len(ser) == 0:\n",
    "        continue\n",
    "    strategy_metrics[name] = {\n",
    "        'Sharpe': sharpe(ser),\n",
    "        'Sortino': sortino(ser),\n",
    "        'Profit Factor': profit_factor(ser),\n",
    "        'Expectancy': expectancy(ser)\n",
    "    }\n",
    "(CSV_DIR / 'strategy_metrics.json').write_text(json.dumps(strategy_metrics, indent=2))\n",
    "strategy_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_metrics = (\n",
    "    merged.groupby('classification')[['roi', 'closed_pnl', 'abs_leverage']]\n",
    "    .agg(['mean', 'std', 'count'])\n",
    ")\n",
    "regime_metrics.columns = ['_'.join([lvl for lvl in col if lvl]).strip('_') for col in regime_metrics.columns.to_flat_index()]\n",
    "regime_metrics = regime_metrics.reset_index().rename(columns={'classification': 'sentiment_classification'})\n",
    "regime_metrics.to_csv(CSV_DIR / 'trader_regime_metrics.csv', index=False)\n",
    "regime_metrics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b83175",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_summary = (\n",
    "    merged.groupby('account')[['roi', 'closed_pnl', 'abs_leverage', 'pnl_positive']]\n",
    "    .agg({'roi': 'mean', 'closed_pnl': 'sum', 'abs_leverage': 'mean', 'pnl_positive': 'mean'})\n",
    ")\n",
    "account_summary = account_summary.rename(columns={'pnl_positive': 'win_rate'})\n",
    "account_summary['composite_score'] = (\n",
    "    account_summary['roi'].rank(pct=True) * 0.4\n",
    "    + account_summary['win_rate'].rank(pct=True) * 0.4\n",
    "    + (1 - account_summary['abs_leverage'].rank(pct=True)) * 0.2\n",
    ")\n",
    "account_summary = account_summary.sort_values('composite_score', ascending=False)\n",
    "account_summary.reset_index().to_csv(CSV_DIR / 'trader_rankings.csv', index=False)\n",
    "account_summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_features = account_summary[['roi', 'win_rate', 'abs_leverage']].fillna(0)\n",
    "scaler_clusters = StandardScaler()\n",
    "cluster_scaled = scaler_clusters.fit_transform(cluster_features)\n",
    "kmeans = KMeans(n_clusters=3, n_init=20, random_state=42)\n",
    "labels = kmeans.fit_predict(cluster_scaled)\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(cluster_scaled)\n",
    "cluster_df = account_summary.reset_index()[['account']].copy()\n",
    "cluster_df['kmeans_cluster'] = labels\n",
    "cluster_df['dbscan_cluster'] = dbscan_labels\n",
    "cluster_df.to_csv(CSV_DIR / 'trader_behavior_clusters.csv', index=False)\n",
    "cluster_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = merged.groupby('account')['abs_leverage'].median().median() * 2\n",
    "alerts = (\n",
    "    merged[merged['abs_leverage'] > threshold]\n",
    "    .groupby('account')['abs_leverage']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'abs_leverage': 'max_leverage_ratio'})\n",
    ")\n",
    "alerts.to_csv(CSV_DIR / 'leverage_alerts.csv', index=False)\n",
    "alerts.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = merged[['roi', 'abs_leverage', 'sentiment_score', 'fee', 'execution_price', 'size_usd']].corr()\n",
    "corr.to_csv(CSV_DIR / 'feature_correlations.csv')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "plt.title('Feature Correlations')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'sentiment_performance_heatmap.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab8de4",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "- Inspect the freshly generated CSV/JSON/PNG artifacts inside this folder.\n",
    "- Update `ds_report.pdf` / `ds_report.txt` with new findings.\n",
    "- Run `notebook_2.ipynb` for supplemental leverage/sentiment diagnostics if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
